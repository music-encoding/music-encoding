<!--
© 2017 by the Music Encoding Initiative (MEI) Council.
  Licensed under the Educational Community License, Version 2.0 (the "License"); you may
  not use this file except in compliance with the License. You may obtain a copy of the License
  at http://opensource.org/licenses/ECL-2.0.

  Unless required by applicable law or agreed to in writing, software distributed under the
  License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS
  OF ANY KIND, either express or implied. See the License for the specific language
  governing permissions and limitations under the License.

  This is a derivative work based on earlier versions of the schema © 2001-2006 Perry Roland
  and the Rector and Visitors of the University of Virginia; licensed under the Educational
  Community License version 1.0.

  CONTACT: info@music-encoding.org                        
                    --><?xml-model href="../validation/mei_odds.rng" type="application/xml" schematypens="http://purl.oclc.org/dsdl/schematron"?><?xml-model href="../validation/mei_odds.rng" type="application/xml" schematypens="http://relaxng.org/ns/structure/1.0"?><TEI xmlns:rng="http://relaxng.org/ns/structure/1.0"
     xmlns:sch="http://purl.oclc.org/dsdl/schematron"
     xmlns="http://www.tei-c.org/ns/1.0"
     version="5.0"
     rend="book"
     xml:lang="en">
   <teiHeader>
      <fileDesc>
         <titleStmt>
            <title level="s">The Music Encoding Initiative Guidelines</title>
            <title level="a">Introduction to MEI</title>
            <respStmt>
               <resp>Edited by</resp>
               <!--TODO: To be filled manually-->
            </respStmt>
            <respStmt>
               <resp>With contributions by</resp>
               <!--TODO: To be filled automatically via GitHub API-->
               <name ref="https://api.github.com/users/bwbohl">Benjamin W. Bohl</name>
               <name ref="https://api.github.com/users/BruxDDay">BruxDDay</name>
               <name ref="https://api.github.com/users/dinamix">dinamix</name>
               <name ref="https://api.github.com/users/ndubo">Norbert Dubowy</name>
               <name ref="https://api.github.com/users/fujinaga">Ichiro Fujinaga</name>
               <name ref="https://api.github.com/users/axgeertinger">Axel Geertinger</name>
               <name ref="https://api.github.com/users/ahankinson">Andrew Hankinson</name>
               <name ref="https://api.github.com/users/irmlindcapelle">irmlindcapelle</name>
               <name ref="https://api.github.com/users/frakel">franz kelnreiter</name>
               <name ref="https://api.github.com/users/kepper">kepper</name>
               <name ref="https://api.github.com/users/zolaemil">Zoltan Komives</name>
               <name ref="https://api.github.com/users/DDMAL-LabManager">DDMAL LabManager</name>
               <name ref="https://api.github.com/users/uliska">Urs Liska</name>
               <name ref="https://api.github.com/users/elsinhadl">Elsa De Luca</name>
               <name ref="https://api.github.com/users/napulen">Néstor Nápoles López</name>
               <name ref="https://api.github.com/users/MajaHartwig">MajaHartwig</name>
               <name ref="https://api.github.com/users/musicEnfanthen">Stefan Münnich</name>
               <name ref="https://api.github.com/users/pe-ro">pe-ro</name>
               <name ref="https://api.github.com/users/lpugin">Laurent Pugin</name>
               <name ref="https://api.github.com/users/JRegimbal">Juliette Regimbal</name>
               <name ref="https://api.github.com/users/rettinghaus">Klaus Rettinghaus</name>
               <name ref="https://api.github.com/users/aseipelt">Agnes Seipelt</name>
               <name ref="https://api.github.com/users/martha-thomae">Martha E. Thomae</name>
               <name ref="https://api.github.com/users/raffazizzi">Raffaele Viglianti</name>
               <name ref="https://api.github.com/users/vigliensoni">Gabriel Vigliensoni</name>
               <name ref="https://api.github.com/users/th-we">Thomas Weber</name>
               <name ref="https://api.github.com/users/musicog">David M. Weigl</name>
            </respStmt>
         </titleStmt>
         <publicationStmt>
            <distributor>Music Encoding Initiative (MEI) Council</distributor>
            <availability>
               <p>
                  <hi>Music Encoding Initiative (MEI)</hi>
               </p>
               <p>NOTICE: Copyright (c) 2018 by the Music Encoding Initiative (MEI) Council.</p>
               <p>Licensed under the Educational Community License, Version 2.0 (the "License"); you may
                                            not use this file except in compliance with the License. You may obtain a copy of the
                                            License at <ref target="http://opensource.org/licenses/ECL-2.0">http://opensource.org/licenses/ECL-2.0</ref>.</p>
               <p>Unless required by applicable law or agreed to in writing, software distributed under
                                            the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
                                            KIND, either express or implied. See the License for the specific language governing
                                            permissions and limitations under the License.</p>
               <p>This is a derivative work based on earlier versions of the schema copyright (c)
                                            2001-2006 Perry Roland and the Rector and Visitors of the University of Virginia;
                                            licensed under the Educational Community License version 1.0.</p>
               <p>CONTACT: contact@music-encoding.org </p>
            </availability>
         </publicationStmt>
      </fileDesc>
   </teiHeader>
   <text>
      <body>
         <div xml:id="introduction" type="div1">
            <head>Introduction to MEI</head>
            <p>Welcome to the MEI Guidelines. They provide documentation for the Music Encoding Initiative's framework for describing music notation documents. This includes both a technical specification of the XML-based implementation of MEI and an explanatory description of its concepts.</p>
            <p>The MEI Guidelines are intended to serve as a reference tool for music encoders. Through the use of natural-language definitions and examples, this documentation assists users of MEI in achieving effective and consistent markup. Despite translating XML and RNG terminology and concepts into more accessible language, it is still a technical one that presupposes a minimal understanding of XML and music notation. Novice encoders may want to start their MEI experience by doing an<ref target="https://music-encoding.org/resources/tutorials.html">introductory tutorial</ref>first. These Guidelines will provide recommendations and arguments for encoding different types of music notation for a variety of purposes. While the specification of the framework is complete, the description is not necessarily complete. MEI is used in various contexts, and not every use-case may be fully reflected in these Guidelines. However, MEI is a community effort, so feedback and suggestions for improvement are highly welcome. Several starting points to get in touch with the MEI community can be found on the<ref target="https://music-encoding.org/community/community-contacts.html">MEI website</ref>.</p>
            <p>These Guidelines make use of real-world examples to illustrate appropriate encoding concepts. We consider the use of such images as fair use. Contributors to these Guidelines are requested to given proper reference to the libraries holding the material used here. They're also asked to be aware of potential copyright infringements and avoid respective material, or replace it with hand-drawn, made-up examples. If you find material that possibly offends copyright, please<ref target="mailto:info@music-encoding.org">get in touch</ref>with us, and we will take it down.</p>
            <div xml:id="about" type="div2">
               <head>About these Guidelines</head>
               <div xml:id="designprinciples" type="div3">
                  <head>MEI Design Principles</head>
                  <p>This section of the Guidelines defines principles and criteria for designing, developing, and maintaining an XML-based encoding scheme for music notation documents.</p>
                  <div xml:id="definitionsandparameters" type="div4">
                     <head>Definitions and Parameters</head>
                     <p>A music notation document is one that contains music notation; that is, any one of a number of "visual analogues of musical sound, either as a record of sound heard or imagined, or as a set of visual instructions for performers." (Ian D. Bent, et al. "Notation." Grove Music Online. Oxford Music Online. 25 May 2010.<ref target="http://www.oxfordmusiconline.com/subscriber/article/grove/music/20114">http://www.oxfordmusiconline.com/subscriber/article/grove/music/20114</ref>{:target='\_blank'}.) However, MEI's understanding is more inclusive than this restrictive definition, i.e. Braille certainly qualifies as music notation documents.</p>
                     <p>The encoding scheme permits both the creation of new music notation documents and the conversion of existing ones from print and other electronic formats. However, conversion of existing documents may require revisions in content or rearrangement of information.</p>
                  </div>
                  <div xml:id="generalprinciples" type="div4">
                     <head>General Principles</head>
                     <p>MEI may be used to encode both primary sources of music notation, such as an autograph or published score, and secondary sources, such as a scholarly edition based on one or more primary sources. The format encompasses both use cases, and the encoder must choose the elements and attributes most appropriate in each case. These Guidelines aim to provide guidance on that task.</p>
                     <p>As an encoded representation of one or more music notation documents, an MEI file may be employed as a surrogate for the original materials.</p>
                     <p>Although the encoding scheme does not define or prescribe intellectual content for music notation documents, it does define content designation and is intended to be used with available data content standards. MEI identifies the essential data elements within music notation documents and establishes codes and conventions necessary for capturing and distinguishing information within those elements for future action or manipulation. While there are a few elements that ought to appear in any MEI document, various intellectual, technical, and economic factors influence the level of detail of analysis and encoding actually undertaken. Taking this into consideration, the encoding scheme is designed with a minimum of required elements and allows for progressively more detailed levels of description as desired.</p>
                     <p>The encoding scheme preserves and enhances the current functionality of existing music notation documents. It permits identification of document structures and content that support description, navigation, analysis, and online and print presentation.</p>
                     <p>The encoding scheme is intended to facilitate interchange between notational tools. It aims to assist in the creation of more effective and consistent encoding, encourage the creation of cooperatively-created and widely available databases of music notation documents, and permit the reuse of encoded data for multiple output purposes. It will also ensure that machine-readable music notation documents will outlive changing hardware and software environments because they are based on a platform-independent standard.</p>
                  </div>
                  <div xml:id="structuralfeatures" type="div4">
                     <head>Structural Features</head>
                     <p>The encoding scheme is based on eXtensible Markup Language (XML), a text-based format for representing structured information. It is expressed as a One Document Does-it-all (ODD) document. For more information on ODD, please refer to TEI Guidelines chapter 22:<ref target="http://www.tei-c.org/release/doc/tei-p5-doc/en/html/TD.html">Documentation Elements</ref>{:target='\_blank'}, chapter 23:<ref target="http://www.tei-c.org/release/doc/tei-p5-doc/en/html/USE.html">Using the TEI</ref>{:target='\_blank'}, and to the TEI's "<ref target="https://tei-c.org/guidelines/customization/getting-started-with-p5-odds/">Getting Started with P5 ODDs</ref>{:target='\_blank'}" document.</p>
                     <p>Related or complementary standards, such as the<ref target="http://www.tei-c.org/Guidelines/P5/">Text Encoding Initiative (TEI) Guidelines for Electronic Text Encoding and Interchange</ref>{:target='\_blank'},<ref target="http://www.loc.gov/ead/">the Encoded Archival Description (EAD)</ref>{:target='\_blank'},<ref target="http://www.loc.gov/marc/bibliographic/ecbdhome.html">MARC 21 Format for Bibliographic Data</ref>{:target='\_blank'}, existing notation encoding schemes, etc. have been consulted and employed as appropriate. For example, the data model includes a header that is comparable to the TEI header, and TEI and EAD naming conventions and tag structures have been used whenever feasible. However, while some feature names are similar, or even the same, it is important to recognize that MEI and TEI have different semantic scope. Obviously, a note element in MEI does not carry the same meaning as the element of the same name in TEI. Perhaps less obviously, a phrase in music notation is unrelated to a textual phrase.</p>
                     <p>With respect to metadata, MEI recognizes the close relationship between the metadata content found in the MEI header and that of catalog records, authority records, and finding aids. Therefore MEI provides ways of indicating in the encoding the corresponding fields of other metadata standards.</p>
                     <p>To ensure broad international and multi-repertoire application of MEI, existing musical terminology was used in building the data model where practical. When appropriate, a more neutral terminology was used to facilitate sharing of concepts and thus stressing the commonalities between different repertoires. Finally, extensive use of attributes and clearly-defined classification mechanisms in the schema permits the refinement of element meanings within specific musical, geographic, or temporal contexts.</p>
                  </div>
                  <div xml:id="controlandmaintenance" type="div4">
                     <head>Control and Maintenance</head>
                     <p>The Music Encoding Initiative Community has given itself<ref target="https://music-encoding.org/community/mei-by-laws.html">By-laws</ref>{:target='\_blank'}, which regulate all essential properties and procedures. The community elects a<ref target="https://music-encoding.org/community/mei-board.html">Board</ref>{:target='\_blank'}, which in turn governs and represents the community. The Board consists of nine elected members, with three seats standing for election for three year terms each year. Everyone registered to the<ref target="https://music-encoding.org/community/community-contacts.html">MEI-L</ref>{:target='\_blank'} mailing list is eligible to vote for the Board.</p>
                     <p>In addition to the Board, there is a<ref target="https://music-encoding.org/community/technical-team.html">Technical Team</ref>{:target='\_blank'}, which is open for anyone interested to work on the maintenance and improvement of MEI itself. The Technical team will assist Interest Groups and other interested community members in an advisory capacity on how to further develop MEI for both existing and new fields of application.</p>
                  </div>
               </div>
               <div xml:id="acknowledgments" type="div3">
                  <head>Acknowledgments</head>
                  <p>Many institutions and individuals assisted in the preparation of these Guidelines and in the overall development of the Music Encoding Initiative framework and community.</p>
                  <p>Grateful acknowledgment is given to the following institutions for their generous contributions: the Akademie der Wissenschaften und der Literatur (AdW) in Mainz for serving as hosting institution for the MEI Community, and the National Endowment for the Humanities (NEH) and the Deutsche Forschungsgemeinschaft (DFG) for their joint financial support of the MEI project in its early stages. We thank several institutions that hosted Music Encoding Conferences or other MEI-related meetings in the past: The AdW Mainz, the University of Virginia Library, the Biblioteca Umanistica of the Università degli Studi Firenze, McGill University Montréal, the Centre d’études supérieures de la Renaissance Tours, the Maryland Institute for Technology in the Humanities (MITH) in College Park, the Oxford e-Research Centre, the Universität Paderborn and the Österreichische Akademie der Wissenschaften Wien in conjunction with the Universität Wien and the Mozarteum Salzburg. We also thank all other institutions that allow their researchers to invest time into both the community and the encoding framework. It is their interest that makes MEI an incredible platform for interchange and scholarly progress.</p>
                  <p>The Text Encoding Initiative is also owed a special debt of gratitude. In addition to providing much of the inspiration for MEI, the TEI organization supplied funding for the MEI Technical Group in its efforts to adopt ODD. The editors of these Guidelines are grateful for those of the TEI, which provided a stellar exemplar and from which we have borrowed shamelessly.</p>
                  <p>MEI has been a community-driven effort for more than a decade, and many individuals have provided significant and much-appreciated commitments of time and energy to the development of MEI: Nikolaos Beer; Vincent Besson; Benjamin W. Bohl; Margrethe Bue; Donald Byrd; Irmlind Capelle; Tim Crawford; David A. Day; Giuliano Di Bacco; Norbert Dubowy; Richard Freedman; Ichiro Fujinaga; Andrew Hankinson; Maja Hartwig; Kristin Herold; Franz Kelnreiter; Johannes Kepper; Robert Klugseder; Zoltán Kömíves; David Lewis; Urs Liska; Elsa De Luca; Erin Mayhood; Stefan Morent; Stefan Münnich; Markus Neuwirth; Kevin Page; Daniel Pitti; Laurent Pugin; Klaus Rettinghaus; Kristina Richts; Daniel Röwenstrunk; Perry Roland; Craig Sapp; Agnes Seipelt; Eleanor Selfridge-Field; Christine Siegert; Peter Stadler; Axel Teich Geertinger; Martha Thomae; Joachim Veit; Raffaele Viglianti; Thomas Weber; and Sonia Wronkowska.</p>
                  <p>Thanks to Bernhard R. Appel; Richard Chesser; Morgan Cundiff; J. Stephen Downie; Oliver Huck; Fotis Jannidis; John Rink; Federica Riva; Frans Wiering and Barbara Wiermann for providing expertise on a wide range of topics related to music notation modelling.</p>
                  <p>Also thanks to Syd Bauman, Terry Catapano, and Sebastian Rahtz for their invaluable problem-solving assistance during the development of the 2010 RNG schema. Thanks to Sebastian Rahtz and James Cummings of the Text Encoding Initiative (TEI) for their help with making ODD work with MEI, their assistance in more closely aligning MEI and TEI, and their quick responses to questions and Roma bug reports.</p>
                  <p>Finally, the members of the Music Encoding Initiative would like to thank Perry Roland for his foresight, engagement and dedication in laying the foundations of this initiative.</p>
               </div>
            </div>
            <div xml:id="basicConcepts" type="div2">
               <head>Basic Concepts of MEI</head>
               <p>This chapter is intended to explain basic concepts of MEI, like events vs. controlevents.</p>
               <div xml:id="musicalDomains" type="div3">
                  <head>Musical Domains</head>
                  <p>The term "music" has many different notions, ranging from audible sounds over written performance instructions or transcriptions of such events to conceptual rulesets that establish different theories of what music is, and what is allowed in music. In 1965, Milton Babbitt distinguished between _graphemic_, _acoustic_ and _auditory_ aspects of music (Babbitt, Milton: _The Use of Computers in Musicological Research_, in: _Perspectives of New Music_ 3/2 (1965), p. 76).</p>
                  <p>Various music encoding formats took up this distinction, most notably SMDL, the _Standard Music Description Language_ (ISO/IEC DIS 10743). While the format itself was hardly ever used for its impractical implementation details, parts of its design certainly influenced the development of other formats, including MEI. In a documentation draft (<ref target="http://xml.coverpages.org/smdl10743-pdf.gz">http://xml.coverpages.org/smdl10743-pdf.gz, p.5</ref>{:target='\_blank'}), SMDL identifies four different _musical domains_:</p>
                  <p>logical domain : The logical domain is the basic musical content – the essence from which all performances and editions of the work are derived, including virtual time values, nominal pitches, etc. The logical domain is describable as “the composer’s intentions with respect to pitches, rhythms, harmonies, dynamics, tempi, articulations, accents, etc.,” and it is the primary focus of SMDL. It can also be described as “the abstract information common to both the gestural and visual domains.” […]</p>
                  <p>gestural domain : The gestural domain is comprised of any number of performances, each of which may specify how and when components of the logical domain is rendered in a specific performance, including all the means whereby the performer actually “expresses” (acoustically instantiates) the music (intonation, agogic and dynamic stress, etc.). The gestural domain is perhaps most succinctly described as “the information added by performers,” or “how the music actually sounds during particular performances.” […]</p>
                  <p>visual domain : The visual domain is comprised of any number of scores, each of which somehow specifies exactly how components of the logical domain is rendered visually in some particular printable (and/or displayable) edition, including such graphical details as symbology, symbol sets, fonts, page layout, beaming conventions and exceptions, etc. The visual domain is perhaps most succinctly described as “the information added by human editors, engravers, and typesetters,” or “how the music actually looks in some particular edition.” […]</p>
                  <p>analytical domain : The analytical domain is comprised of any number of theoretical analyses and/or commentaries, each of which somehow specifies opinions, exegeses, etc. about any or all of the information in the other three domains. […]</p>
                  <p>On a generic level, MEI follows the same definition, and it definitely shares the same terminology. However, not all four domains are available throughout the MEI schema, and quite frequently, two domains fall together in MEI. Very often, _MEI prioritizes the visual domain over the gestural domain_ by (partly) _conflating the logical and the visual domains_. For example, MEI utilizes the &lt;code&gt;@pname&lt;/code&gt; (pitch name) attribute on notes to capture the _written_ pitch of a note, whereas the sounding pitch may be described with the &lt;code&gt;@pname.ges&lt;/code&gt; attribute. Here, the logical and visual domains go without a special indication, whereas the gestural domain is identified by a special suffix. However, in case of transposing instruments, additional markup (namely the attributes &lt;code&gt;@trans.diat&lt;/code&gt; and &lt;code&gt;@trans.semi&lt;/code&gt; from MEI's attribute class <ident type="class">att.staffDef.log</ident>) will create a distinction between the logical and visual domain (see chapter <ptr target="#cmnDefs"/>). In that case, &lt;code&gt;@pname&lt;/code&gt; will be restricted to the visual domain, while the logical aforementioned attributes provide additional information for the logical domain.</p>
                  <p>Even though the technical implementation of MEI prioritizes the visual domain to some degree, this does not mean that any given encoding has to provide visual information. MEI takes no assumption on what data is required: While an OMR project (_optical music recognition_) may generate strictly visually oriented data only, another project focussed on audio transcriptions may generate gestural data only. A third project could integrate both approaches.</p>
                  <p>In order to avoid ambiguous encodings, MEI is very strict and specific on the scope of its individual markup elements. For an encoder, the suffixes mentioned above provide clear hints on which domain is addressed by specific markup: Many attributes carry a suffixed _.log_ (logical), _.ges_ (gestural), _.vis_ (visual), or _.anl_ (analytical) in their name. In addition, the internal structure of MEI heavily relies on those different domains. When customizing MEI (see chapter <ptr target="#meicustomization"/>), it is possible to turn off either visual or gestural domain encoding completely. That way, MEI allows to address the four most eminent musical domains specifically and independent of each other.</p>
               </div>
               <div xml:id="eventsControlevents" type="div3">
                  <head>Events and Controlevents</head>
                  <p>MEI differentiates between two essential aspects of music notation: <hi rend="italic">Events</hi> and <hi rend="italic">ControlEvents</hi>. There are other examples for such a separation of concerns with regard to music. In Greg's Copy-Text Theory (W.W.Greg: <hi rend="italic">The Rationale of Copy-Text</hi>, 1950), a distinction between primary and secondary text is made; similar attempts have been made for music specifically.</p>
                  <p>In MEI, elements describing the basic musical text are referred to as <hi rend="italic">Events</hi>. They are the building blocks for the stream of music – mostly those are <gi scheme="MEI">note</gi>s, <gi scheme="MEI">rest</gi>s, and <gi scheme="MEI">chord</gi>s. In contrast, <hi rend="italic">ControlEvents</hi> make no independent contribution to that flow of music. Instead, they provide additional information about the encoded <hi rend="italic">Events</hi>, they <hi rend="italic">control</hi> their performance. Examples for such <hi rend="italic">ControlEvents</hi> are <gi scheme="MEI">dynam</gi>ic markings, <gi scheme="MEI">tempo</gi>s indications, or performance <gi scheme="MEI">dir</gi>ectives. Depending on the encoding strategy used, <gi scheme="MEI">slur</gi>s and <gi scheme="MEI">tie</gi>s often also fall into this category (they may be encoded as attributes instead, in which case they become a property of the basic events). Simply put, <hi rend="italic">Events</hi> describe <hi rend="bold italic">what</hi> needs to be performed, and <hi rend="italic">ControlEvents</hi> indicate <hi rend="bold italic">how</hi> it needs to be performed. In (<ptr target="#cmn"/>-based) MEI, <hi rend="italic">Events</hi> are nested inside a <gi scheme="MEI">layer</gi> element, while <hi rend="italic">ControlEvents</hi> are direct children of the first <gi scheme="MEI">measure</gi> they apply to, following all <gi scheme="MEI">staff</gi> elements there. These structural differences result in different markup concepts. As <hi rend="italic">Events</hi> are encoded inside <gi scheme="MEI">layer</gi>s, their <hi rend="italic">semantic position</hi> inside the encoded work can be derived from their <hi rend="italic">structural position</hi> – the measure, staff and layer they're nested in, and within that layer by their position inside the sequence of all layer children. As mentioned above, it is highly <hi rend="italic">recommended</hi> to encode <hi rend="italic">ControlEvents</hi> inside the first measure they apply to, but they still require references to the actual events they apply to. There are two common concepts to provide such a connection, both of which offering specific benefits and drawbacks. A technically very stable connection between <hi rend="italic">ControlEvents</hi> and <hi rend="italic">Events</hi> can be established by using <hi rend="italic">pointers</hi>. In this case, all events that need to be referenced need an <att>xml:id</att> attribute, which holds a globally unique identifier for this very element. The referencing controlevent then uses a <att>startid</att> and, if necessary, <att>endid</att> attribute to create a link to where in the stream of music it is supposed to start or end.</p>
                  <p>
                     <figure>
                        <head/>
                        <egXML xmlns="http://www.tei-c.org/ns/Examples" rend="code" xml:space="preserve"><xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="../examples/shared/controlevents-dynam1.txt" parse="text/plain"/></egXML>
                     </figure>
                  </p>
                  <p>In the example above, the <gi scheme="MEI">dynam</gi> element references the second quarter in the given measure. Additional attributes like <att>place</att> may be used to describe the position of the <hi rend="italic">forte</hi> indication within the score. A <gi scheme="MEI">hairpin</gi> element may use the <att>endid</att> attribute to indicate the duration of the hairpin using the same mechanism as above.</p>
                  <p>
                     <specList>
                        <specDesc key="att.startId" atts="startid"/>
                        <specDesc key="att.startEndId" atts="endid"/>
                     </specList>
                  </p>
                  <p>A <hi rend="italic">ControlEvent</hi> encoded like above will be strictly tied to the referenced <hi rend="italic">Events</hi> – if their position inside the XML document changes for whatever reason, they will keep that connection. This means that the <hi rend="italic">semantic position</hi> to which they are bound may change without affecting the binding. An example could be an inserted additional note in front – the dynamic marking would not start on the second quarter, but perhaps on the third instead.</p>
                  <p>As this behavior may not be desired in all cases, an alternative binding between <hi rend="italic">ControlEvents</hi> and <hi rend="italic">Events</hi> is possible, relying on <hi rend="italic">timestamps</hi> instead. This mechanism is illustrated in the following example:</p>
                  <p>
                     <figure>
                        <head/>
                        <egXML xmlns="http://www.tei-c.org/ns/Examples" rend="code" xml:space="preserve"><xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="../examples/shared/controlevents-dynam2.txt" parse="text/plain"/></egXML>
                     </figure>
                  </p>
                  <p>Here, no <att>xml:id</att> is required on notes. Instead, the <gi scheme="MEI">dynam</gi> element uses the <att>staff</att> and <att>layer</att> attributes to indicate to which set of events the following <att>tstamp</att> attribute refers to.</p>
                  <p>
                     <specList>
                        <specDesc key="att.timestamp.logical" atts="tstamp"/>
                     </specList>
                  </p>
                  <p>This mechanism actually depends on what has been only recommended above: placing the controlevent inside the measure where it starts. The <att>startid</att> reference mechanism would work equally well if all controlevents where positioned in the very first or last measure, or actually even inside a separate file. The <att>tstamp</att> references however would not, they depend on correct placement of the controlevents inside the XML tree. For consistency, it is therefore <hi rend="italic">recommended</hi> to always use this placement.</p>
                  <p>The benefit of this concept is that controlevents are tied to a <hi rend="italic">semantic position</hi>, but not necessarily to a given XML element. The <hi rend="italic">forte</hi> may still be placed on the second quarter, even though the composer may have replaced that quarter G4 with a different pitch and / or duration. Actually, it is not required that an <hi rend="italic">Event</hi> can be found at the position indicated by a timestamp. This may be useful to encode a slur ending at an arbitrary position between two events, or dynam markings spread across otherwise empty measures.</p>
                  <p>If the ending of a <hi rend="italic">ControlEvent</hi> shall be given by timestamp, the <att>tstamp2</att> attribute is used.</p>
                  <p>
                     <specList>
                        <specDesc key="att.timestamp2.logical" atts="tstamp2"/>
                     </specList>
                  </p>
                  <p>Because of potential inconsistencies, an encoding should not offer both <att>startid</att> and <hi rend="italic">tstamp</hi> or <att>endid</att> and <att>tstamp2</att>. Though not being recommendable, it is possible to mix <att>startid</att> with <att>tstamp2</att> and <att>tstamp</att> with <att>endid</att>. In general, it is easier for software to process <att>startid</att> and <att>endid</att>. When no other arguments apply, using <att>xml:id</att>-based pointers is therefore the most common way to connect <hi rend="italic">ControlEvents</hi> with <hi rend="italic">Events</hi>.</p>
                  <p>The details on how timestamps are calculated and used in MEI are given in <ptr target="#timestamps"/>.</p>
               </div>
               <div xml:id="timestamps" type="div3">
                  <head>Timestamps in MEI</head>
                  <p>In MEI, timestamps are treated in a slightly simplified way: they have no notion of <hi rend="italic">beat</hi>. Instead, timestamps rely solely on the numbers given in the meter signature. In a measure of 4/4, timestamps will range from 1 to 4. The second eighth note will be 1.5 in this case. If the same measure would be given in 2/2, it would be 1.25 instead.</p>
                  <p>
                     <specList>
                        <specDesc key="att.timestamp.logical" atts="tstamp"/>
                     </specList>
                  </p>
                  <p>At this point, MEI uses real numbers only to express timestamps. In case of (nested or complex) tuplets, this solution is inferior to fractions because of rounding errors. It is envisioned to introduce a fraction-based value for timestamps in a future revision of MEI. For now, it is recommended to round the fractional part of the number to no more than five digits to avoid such problems.</p>
                  <p>Durations may also be expressed based on timestamps. In this case, the values are a combination of the <hi rend="italic">count of measures</hi> that need to be moved forward to reach the measure in which an encoded feature ends, and the <hi rend="italic">timestamp</hi> within that measure.</p>
                  <p>
                     <specList>
                        <specDesc key="att.timestamp2.logical" atts="tstamp2"/>
                     </specList>
                  </p>
                  <p>The following example contains a number of <gi scheme="MEI">slur</gi> examples illustrating durations expressed by timestamps.</p>
                  <p>
                     <figure>
                        <head/>
                        <egXML xmlns="http://www.tei-c.org/ns/Examples" rend="code" xml:space="preserve"><xi:include xmlns:xi="http://www.w3.org/2001/XInclude" href="../examples/shared/timestamp-durations1.txt" parse="text/plain"/></egXML>
                     </figure>
                  </p>
                  <p>Sometimes, timestamps are used to indicate positions where no music <hi rend="italic">Events</hi> are located (see <ptr target="#eventsControlevents"/>). Therefore, the allowed range of timestamps stretches from 0 to the current meter count + 1. By definition, a timestamp of 1 indicates the position of the left barline, while a timestamp of 5 (in case of a 4/4 meter) indicates the right barline. This makes it possible to encode open-ended slurs in a graphical way. However, it should be kept in mind that such timestamps may not be converted to <att>startid</att> and <att>endid</att>, and not every application may be able to render them correctly, even though they are perfectly valid MEI, and sometimes are necessary to faithfully transcribe a source.</p>
               </div>
               <div xml:id="meiprofiles" type="div3">
                  <head>MEI Profiles</head>
                  <p>MEI is an encoding framework, not a data format. This means that MEI provides recommendations for encoding music documents, but it depends on the encoder's needs and requirements to which features and solutions are appropriate to the task and should to be used. MEI offers specific models for different notation types and music repertoires, but it is rarely advisable to use them all side by side in one encoding.</p>
                  <p>In order to use MEI, it is advised to use a restricted version of the schema, which will make it easier both for an encoder and a reader of the encoded files. MEI provides a number of pre-defined _profiles_, which focus on specific uses of MEI while still maintaining a great level of flexibility. For projects that need even better control over their data, it is highly recommended to create a more specific customized version of MEI (see chapter <ptr target="#meicustomization"/>). The following customizations are provided with every release of MEI:</p>
                  <p>mei-CMN : For most users, this will be the best starting point into music encoding with MEI. The _mei-CMN_ customization targets at documents that use _Common Western Music Notation_. The specific rules for that notation are specified in chapter <ptr target="#cmn"/>, even though other chapters of these Guidelines apply as well.</p>
                  <p>mei-Mensural : For documents written in _Mensural Notation_ (both black and white), MEI offers the _mei-Mensural_ customization. The specific rules for that notation are specified in chapter <ptr target="#mensural"/>, even though other chapters of these Guidelines apply as well.</p>
                  <p>mei-Neumes : This profile allows to encode medieval _Neume Notation_ with MEI. The specific rules for that notation are specified in chapter <ptr target="#neumes"/>, even though other chapters of these Guidelines apply as well. Please note that the _mei-Neumes_ profile has undergone significant changes from MEI version 3 to version 4.</p>
                  <p>mei-Basic : As an encoding framework, MEI offers multiple approaches to encode certain features at various levels of detail. While this flexibility is at the core of MEI and often required for research projects, it is an obstacle when developing software and converters for MEI. The _mei-Basic_ profile is a subset of MEI which restricts it to one way of encoding for every feature of music notation. It covers _Common Western Music Notation_ only, and excludes all editorial markup. In essence, it has the same functionality as most other music encoding formats like MusicXML or MNX. The purpose of _mei-Basic_ is to serve as common ground for data interchange, both between projects using different profiles of MEI, and other encoding schemes.</p>
                  <p>mei-all : This is the full definition of MEI. It includes all different repertoires, which has certain side effects and enables encoding options that are neither intended nor advocable. For example, in mensural notation music is organized by staves. In contrast, Common Music Notation utilizes measures, which in turn contain staves. These staves have a different meaning here, and are modeled differently in MEI. _mei-all_ mixes those models and thus invites encoding errors. In general, you should almost never use _mei-all_ except for testing purposes.</p>
                  <p>mei-all_anyStart : This profile includes all of _mei-all_, but extends it even further so that it allows any MEI element as root of conforming MEI instances. In regular MEI, the only allowed starting elements are <gi scheme="MEI">mei</gi>, <gi scheme="MEI">meiHead</gi>, <gi scheme="MEI">music</gi> and <gi scheme="MEI">meiCorpus</gi>. The sole purpose of this customization is to simplify validation at tutorial sessions and other educational purposes. It should not be used in production.</p>
                  <p>The first three profiles provide good starting points to encode music from the respective repertoires. They may also serve as template for further, project-specific customizations. The latter two profiles are targetting very specific use cases and should not be used by default.</p>
               </div>
               <div xml:id="meicustomization" type="div3">
                  <head>Customizing MEI</head>
                  <p>In production, it is best to use a customized version of MEI, restricted to the very needs of a project. Such a custom schema will guide the encoders and will help to ensure consistency and data quality throughout a project's files. A customization typically provides a subset of MEI's encoding models (typically starting from one of the official _profiles_ mentioned in chapter <ptr target="#meiprofiles"/>), with only one solution for any given situation being allowed. The customization will help to reflect the scope of a project into its data: Only those aspects of music notation a project is interested in will be allowed, so that the absence of a specific information can not be misunderstood as an oversight of the encoders. Larger editorial projects like _Complete Works_ editions typically use _Editorial Guidelines_ (german: _Editionsrichtlinien_) for the same purposes: (internal) quality control and (external) documentation. In that sense, MEI customizations may serve as Editorial Guidelines in digital form.</p>
                  <p>MEI is implemented in ODD. ODD, or _One Document Does-it-all_, is another XML-based markup language developed and maintained by the TEI. TEI's documentation for ODD can be found in the TEI Guidelines chapter 22:<ref target="http://www.tei-c.org/release/doc/tei-p5-doc/en/html/TD.html">Documentation Elements</ref>{:target='\_blank'}, chapter 23:<ref target="http://www.tei-c.org/release/doc/tei-p5-doc/en/html/USE.html">Using the TEI</ref>{:target='\_blank'}, and the "<ref target="https://tei-c.org/guidelines/customization/getting-started-with-p5-odds/">Getting Started with P5 ODDs</ref>{:target='\_blank'}" document.</p>
                  <p>At this point, there is no specific documentation on how to customize MEI with ODD beyond the generic TEI documentation. However, the provided <ptr target="#meiprofiles"/> are based on ODD customizations, and may serve as starting point for further project-specific restrictions. They can be found at<ref target="https://github.com/music-encoding/music-encoding/tree/develop/customizations">https://github.com/music-encoding/music-encoding/tree/develop/customizations</ref>{:target='\_blank'}. In addition, several projects have shared their customizations on GitHub, such as<ref target="https://github.com/Freischuetz-Digital/data-music/tree/master/schemata/odd">Freischütz Digital</ref>{:target='\_blank'} or<ref target="https://github.com/BeethovensWerkstatt/module2/tree/dev/data/odd">Beethovens Werkstatt</ref>{:target='\_blank'}.</p>
                  <p>MEI provides a webservice at<ref target="http://custom.music-encoding.org/">http://custom.music-encoding.org</ref>{:target='\_blank'} which allows to compile such customizations against the MEI sources in order to generate RelaxNG schemata, which can be used for validation. More documentation on customizing MEI will be provided as time permits; until then, it is recommended to<ref target="https://music-encoding.org/community/community-contacts.html">reach out to the MEI Community</ref>{:target='\_blank'} for additional assistance.</p>
               </div>
            </div>
            <div xml:id="samplesTools" type="div2">
               <head>Sample Encodings and Tools for MEI</head>
               <p>The Music Encoding Initiative provides a collection of sample encodings, which demonstrate a wide-range of uses of MEI in real-world contexts. They are available from<ref target="https://github.com/music-encoding/sample-encodings">https://github.com/music-encoding/sample-encodings</ref>{:target='\_blank'}.</p>
               <p>For MEI, there is also a number of tools, which facilitate encoding of and working with MEI instances in various contexts. These tools are available from the<ref target="https://music-encoding.org/resources/tools.html">https://music-encoding.org/resources/tools.html</ref>website.</p>
            </div>
         </div>
      </body>
   </text>
</TEI>
